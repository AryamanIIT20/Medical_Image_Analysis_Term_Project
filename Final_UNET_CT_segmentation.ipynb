{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ad630-84a8-4384-8050-ca1283da620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from timeit import default_timer as timer\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import cv2\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "%config InlineBackend.figure_format = \"svg\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(f\"Using device:{device}\")\n",
    "print(f\"Using {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9ce79-a9ad-49b8-b2fd-232f6a2eb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [file for file in os.listdir(root_dir) if file.startswith('image_') and file.endswith('.jpg')]\n",
    "        self.mask_files = [file for file in os.listdir(root_dir) if file.startswith('label_') and file.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        mask_name = img_name.replace('image_', 'label_').replace('.jpg', '.png')\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        mask_path = os.path.join(self.root_dir, mask_name)\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert to grayscale mask\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "# Define transformations for the images and masks\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "    transforms.ToTensor()            # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Create the custom train dataset\n",
    "custom_dataset = CustomDataset(root_dir=\"CT_train\", transform=transform)\n",
    "train_size = int(0.8 * len(custom_dataset))  # 80% of the data for training\n",
    "val_size = len(custom_dataset) - train_size  # Remaining 20% for validation\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n",
    "\n",
    "# Print the number of samples in the train dataset\n",
    "print(\"Number of samples in the train dataset:\", len(train_dataset))\n",
    "print(\"Number of samples in the validation dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae3541-2ac7-412a-82f6-f2118f012a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(0, len(train_dataset), size = [1,]).item()\n",
    "img,mask = train_dataset[random_idx]\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img.squeeze(), cmap = \"gray\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask.squeeze(), cmap = \"gray\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "img.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fa4a4-2395-4ed4-8a6f-7132f350c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d7832-5e45-45a2-a9b5-06dd23a27d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cd57f-fd6b-4d94-97b1-237d2cf4a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scSE(nn.Module):\n",
    "    def __init__(self, in_channels, r = 2):\n",
    "        super(scSE, self).__init__()\n",
    "        mid_channels = in_channels//r\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(mid_channels, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = 1, kernel_size = 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Take mean over H and W dimensions\n",
    "        mean_tensor = torch.mean(x, dim=[2, 3], keepdim=True)\n",
    "        channel_scaling = self.cSE(mean_tensor).expand(-1, -1, x.size(2), x.size(3))\n",
    "        u_cSE = x*channel_scaling\n",
    "        spatial_scaling = self.sSE(x).expand(-1, x.size(1), -1, -1)\n",
    "        u_sSE = x*spatial_scaling\n",
    "        return torch.maximum(u_sSE,u_cSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989be444-c205-4d2c-96a4-3f86dca5c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        # self.SE1 = scSE(16)\n",
    "        # self.SE2 = scSE(32)\n",
    "        # self.SE3 = scSE(64)\n",
    "        # self.SE4 = scSE(128)\n",
    "        # self.SE5 = scSE(64)\n",
    "        # self.SE6 = scSE(32)\n",
    "        # self.SE7 = scSE(16)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x1 = self.SE1(self.inc(x))\n",
    "        # x2 = self.SE2(self.down1(x1))\n",
    "        # x3 = self.SE3(self.down2(x2))\n",
    "        # x4 = self.SE4(self.down3(x3))\n",
    "        # x5 = self.down4(x4)\n",
    "        # x = self.SE5(self.up1(x5, x4))\n",
    "        # x = self.SE6(self.up2(x, x3))\n",
    "        # x = self.SE7(self.up3(x, x2))\n",
    "\n",
    "        x1 = (self.inc(x))\n",
    "        x2 = (self.down1(x1))\n",
    "        x3 = (self.down2(x2))\n",
    "        x4 = (self.down3(x3))\n",
    "        x5 = self.down4(x4)\n",
    "        x = (self.up1(x5, x4))\n",
    "        x = (self.up2(x, x3))\n",
    "        x = (self.up3(x, x2))\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SEUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.SE1 = scSE(16)\n",
    "        self.SE2 = scSE(32)\n",
    "        self.SE3 = scSE(64)\n",
    "        self.SE4 = scSE(128)\n",
    "        self.SE5 = scSE(64)\n",
    "        self.SE6 = scSE(32)\n",
    "        self.SE7 = scSE(16)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.SE1(self.inc(x))\n",
    "        x2 = self.SE2(self.down1(x1))\n",
    "        x3 = self.SE3(self.down2(x2))\n",
    "        x4 = self.SE4(self.down3(x3))\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.SE5(self.up1(x5, x4))\n",
    "        x = self.SE6(self.up2(x, x3))\n",
    "        x = self.SE7(self.up3(x, x2))\n",
    "\n",
    "        # x1 = (self.inc(x))\n",
    "        # x2 = (self.down1(x1))\n",
    "        # x3 = (self.down2(x2))\n",
    "        # x4 = (self.down3(x3))\n",
    "        # x5 = self.down4(x4)\n",
    "        # x = (self.up1(x5, x4))\n",
    "        # x = (self.up2(x, x3))\n",
    "        # x = (self.up3(x, x2))\n",
    "        \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a86d79-7d37-4615-8282-0a26d4bbcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "n_classes = 2\n",
    "model = SEUNet(n_channels,n_classes,bilinear = True).to(device)\n",
    "summary(model, input_size = [8,1,512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ba7d0-89d8-4ac4-82e5-3b56665ad603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_class_weights(dataset):\n",
    "#     # Initialize counters for each class\n",
    "#     class_counts = {0:0,1:0}\n",
    "    \n",
    "#     # Iterate through the dataset to count pixel occurrences\n",
    "#     for _, labels in tqdm(dataset):\n",
    "#         # Flatten the label tensor and count occurrences\n",
    "#         unique, counts = torch.unique(labels, return_counts=True)\n",
    "#         for i,value in enumerate(unique):\n",
    "#             class_counts[value.item()] +=counts[i]\n",
    "    \n",
    "#     # Calculate inverse frequencies\n",
    "#     class_freq = np.zeros((2,))\n",
    "#     total = 0\n",
    "#     for i in range(2):\n",
    "#         class_freq[i] = class_counts[i]\n",
    "#         total = total + class_counts[i]\n",
    "        \n",
    "#     class_weights = total/class_freq\n",
    "#     class_weights = class_weights/torch.sum(class_weights)\n",
    "\n",
    "#     return class_weights.float()\n",
    "\n",
    "# # Example usage:\n",
    "# class_weights = calculate_class_weights(train_dataset)\n",
    "# print(\"Class Weights:\", class_weights)\n",
    "# class_weights = class_weights.to(device)\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96f369-d475-413b-bf8b-9d05105f366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Reshape logits to [batch_size * height * width, num_classes]\n",
    "        logits_flat = logits.permute(0, 2, 3, 1).contiguous().view(-1, logits.shape[1])\n",
    "        \n",
    "        # Reshape labels to [batch_size * height * width]\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        # Calculate cross entropy loss\n",
    "        if self.weight is not None:\n",
    "            # Apply class weights if provided\n",
    "            loss = F.cross_entropy(logits_flat, labels_flat, weight=self.weight)\n",
    "        else:\n",
    "            # Without class weights\n",
    "            loss = F.cross_entropy(logits_flat, labels_flat)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1e445-29fe-4914-b57c-06293da659c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyDiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weight_ce=None, smooth_dice=1.):\n",
    "        super(CrossEntropyDiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.weight_ce = weight_ce\n",
    "        self.smooth_dice = smooth_dice\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Reshape logits to [N*H*W, C] and targets to [N*H*W]\n",
    "        logits = logits.permute(0, 2, 3, 1).contiguous().view(-1, self.num_classes)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.weight_ce)\n",
    "\n",
    "        # Reshape logits back to [N, H, W, C]\n",
    "        logits = logits.view(-1, logits.size(-1)).view(logits.size(0), -1, logits.size(-1)).permute(0, 2, 1)\n",
    "\n",
    "        # Dice loss\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        dice_loss = 0.\n",
    "\n",
    "        for class_idx in range(1, self.num_classes):  # Exclude background class\n",
    "            class_probs = probs[:, class_idx]\n",
    "            class_targets = (targets == class_idx).float()\n",
    "\n",
    "            # Flatten probabilities and targets\n",
    "            class_probs_flat = class_probs.view(-1)\n",
    "            class_targets_flat = class_targets.view(-1)\n",
    "\n",
    "            # Calculate intersection and union\n",
    "            intersection = torch.sum(class_probs_flat * class_targets_flat)\n",
    "            union = torch.sum(class_probs_flat) + torch.sum(class_targets_flat)\n",
    "\n",
    "            # Calculate Dice coefficient\n",
    "            dice_coefficient = (2. * intersection + self.smooth_dice) / (union + self.smooth_dice)\n",
    "\n",
    "            # Add class Dice loss to the total loss\n",
    "            dice_loss += 1. - dice_coefficient\n",
    "\n",
    "        # Average over classes\n",
    "        dice_loss /= (self.num_classes - 1)\n",
    "\n",
    "        # Combine cross-entropy and Dice losses\n",
    "        loss = ce_loss + dice_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "# # Example usage:\n",
    "# logits = torch.randn(2, 4, 128, 128)  # Example logits (4 classes including background)\n",
    "# targets = torch.randint(0, 4, (2, 128, 128))  # Example targets (multi-class masks)\n",
    "\n",
    "# # Define class weights for cross-entropy (optional)\n",
    "# class_weights = torch.tensor([0.5, 1.0, 1.0, 1.5])  # Example class weights\n",
    "\n",
    "# # Create an instance of CrossEntropyDiceLoss\n",
    "# loss_function = CrossEntropyDiceLoss(num_classes=4, weight_ce=class_weights)\n",
    "\n",
    "# # Calculate loss\n",
    "# loss = loss_function(logits, targets)\n",
    "\n",
    "# print(loss.item())  # Print the calculated loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8d056-4f6c-45f2-829d-b287049b0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "n_classes = 2\n",
    "model = SEUNet(n_channels,n_classes,bilinear = True).to(device)\n",
    "class_weights = torch.tensor([0.1,0.9])\n",
    "loss_fn = CrossEntropyDiceLoss(num_classes = n_classes,weight_ce = class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b81da-01ff-4e5f-b445-6d4e4bc17a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(0, len(train_dataset), size = [1,]).item()\n",
    "image,mask = train_dataset[random_idx]\n",
    "image = image.to(device)\n",
    "pred = model(image.unsqueeze(dim = 0)).squeeze()\n",
    "labels = pred.squeeze().argmax(dim = 0).detach()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(labels.cpu(), cmap = \"jet\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask.squeeze(), cmap = \"jet\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870330d-b752-4bc2-8e72-fead88b0862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_time(start, end, device=None):\n",
    "    total_time = end - start\n",
    "    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2d2d1-aa0d-4618-84b0-c166556f7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "n_classes = 2\n",
    "model = SEUNet(n_channels,n_classes,bilinear = True).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "class_weights = torch.tensor([0.1,0.9]).to(device)\n",
    "loss_fn = CrossEntropyDiceLoss(num_classes = n_classes,weight_ce = class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.004)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 4, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f19a9-34bd-4862-8f9b-a13e29489e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dcbf3-d9a2-461d-bdc2-f3e92cc69161",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last = True)\n",
    "len(train_dataloader), len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f881907-096f-443d-bd97-72123debe0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_start = timer()\n",
    "train_loss_vec = []\n",
    "val_loss_vec = []\n",
    "epoch_vec = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"Epoch {epoch}\\n-----------\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i,(img,mask) in enumerate(train_dataloader):\n",
    "        if i%10 == 0:\n",
    "            print(f\"Train Mini-batch {i}\")\n",
    "        img,mask = img.to(device), mask.long().to(device)\n",
    "        pred = model(img)\n",
    "        loss = loss_fn(pred,mask.squeeze())\n",
    "        train_loss+=loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss/=len(train_dataloader)\n",
    "    train_loss_vec.append(train_loss.detach().cpu())\n",
    "    epoch_vec.append(epoch)\n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i,(val_img, val_mask) in enumerate(val_dataloader):\n",
    "            if i%10==0:\n",
    "                print(f\"Validation Mini-batch {i}\")\n",
    "            val_img,val_mask = val_img.to(device), val_mask.long().to(device)\n",
    "            val_pred = model(val_img)\n",
    "            val_loss_step = loss_fn(val_pred,val_mask.squeeze())\n",
    "            val_loss += val_loss_step\n",
    "                \n",
    "        val_loss/=len(val_dataloader)\n",
    "        val_loss_vec.append(val_loss.cpu())\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Training Loss = {train_loss:.4f} | Validation Loss = {val_loss:.4f}\")\n",
    "\n",
    "train_stop = timer()\n",
    "time_taken = print_train_time(train_start, train_stop, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dfd45-6472-422b-b253-867abf8bbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(0, len(val_dataset), size = [1,]).item()\n",
    "image,mask = val_dataset[random_idx]\n",
    "image = image.to(device)\n",
    "pred = model(image.unsqueeze(dim = 0)).squeeze()\n",
    "labels = pred.squeeze().argmax(dim = 0).detach()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(labels.cpu(), cmap = \"gray\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask.squeeze(), cmap = \"gray\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92707e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_curves(train_loss_vec,val_loss_vec,epoch_vec):\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_vec,label = \"training loss\")\n",
    "    plt.plot(val_loss_vec,label = \"validation loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_loss_curves(train_loss_vec,val_loss_vec,epoch_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba199d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents = True, exist_ok = True)\n",
    "MODEL_NAME = \"second_unet.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "#torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_model = SEUNet(n_channels = 1,n_classes = 2,bilinear = True).to(device)\n",
    "se_model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(0, len(train_dataset), size = [1,]).item()\n",
    "image,mask = train_dataset[random_idx]\n",
    "image = image.to(device)\n",
    "pred = se_model(image.unsqueeze(dim = 0)).squeeze()\n",
    "labels = pred.squeeze().argmax(dim = 0).detach()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(labels.cpu(), cmap = \"jet\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask.squeeze(), cmap = \"jet\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11837d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents = True, exist_ok = True)\n",
    "MODEL_NAME = \"first_unet.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_se_model = UNet(n_channels,n_classes,bilinear = True).to(device)\n",
    "non_se_model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60db7-0baf-44c2-9b11-8cf9c2b833e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(prediction, target, class_index):\n",
    "    smooth = 1e-7\n",
    "    prediction = prediction == class_index\n",
    "    target = target == class_index\n",
    "\n",
    "    intersection = torch.sum(prediction & target).item()\n",
    "    union = torch.sum(prediction | target).item()\n",
    "\n",
    "    jaccard_score = (intersection + smooth) / (union + smooth)\n",
    "    return jaccard_score\n",
    "\n",
    "def compute_average_jaccard_coefficient(model, dataset, num_classes):\n",
    "    model.eval()\n",
    "    class_jaccard_scores = np.zeros(num_classes)\n",
    "    with torch.inference_mode():\n",
    "        for inputs, targets in tqdm(dataset):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.unsqueeze(0)).squeeze().argmax(dim = 0)\n",
    "\n",
    "            for class_index in range(num_classes):\n",
    "                class_jaccard_scores[class_index] += jaccard_coefficient(outputs, targets, class_index)\n",
    "\n",
    "    average_jaccard_scores = class_jaccard_scores / len(dataset)\n",
    "    return average_jaccard_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01722b-7055-4621-bb91-acfe35162878",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "se_jaccard_scores = compute_average_jaccard_coefficient(se_model, val_dataset, num_classes)\n",
    "print(se_jaccard_scores)\n",
    "se_jaccard_avg = np.mean(se_jaccard_scores[1:])\n",
    "print(se_jaccard_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991879bc-b1bc-4fe4-bb5f-837126edb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "non_se_jaccard_scores = compute_average_jaccard_coefficient(non_se_model, val_dataset, num_classes)\n",
    "print(non_se_jaccard_scores)\n",
    "non_se_jaccard_avg = np.mean(non_se_jaccard_scores[1:])\n",
    "print(non_se_jaccard_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(0, len(val_dataset), size = [1,]).item()\n",
    "image,mask = val_dataset[random_idx]\n",
    "image = image.to(device)\n",
    "pred_non_se = non_se_model(image.unsqueeze(dim = 0)).squeeze()\n",
    "pred_se = se_model(image.unsqueeze(dim = 0)).squeeze()\n",
    "labels_non_se = pred_non_se.squeeze().argmax(dim = 0).detach()\n",
    "labels_se = pred_se.squeeze().argmax(dim = 0).detach()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(labels_non_se.cpu(), cmap = \"gray\")\n",
    "plt.title(\"Non SE\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(labels_se.cpu(), cmap = \"gray\")\n",
    "plt.title(\"SE\")\n",
    "plt.axis(False)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask.squeeze(), cmap = \"gray\")\n",
    "plt.title(\"GT\")\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
